{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1mYsTMVppxdc"
   },
   "source": [
    "# START"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5040,
     "status": "ok",
     "timestamp": 1619802892526,
     "user": {
      "displayName": "Nelson Salazar",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhV5m5l7Buk-Ru2abgDPbFAk7ZlpSSCm1lxnxYz=s64",
      "userId": "11090170720674283185"
     },
     "user_tz": 300
    },
    "id": "qWJXRGxU1YTz"
   },
   "outputs": [],
   "source": [
    "#Set-up\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Built-in Python Modules\n",
    "import datetime\n",
    "import inspect\n",
    "import os\n",
    "import csv\n",
    "import math\n",
    "import random\n",
    "import warnings\n",
    "from calendar import monthrange\n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "#Python add-ons\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "from matplotlib import dates as mpl_dates\n",
    "from tqdm import tqdm\n",
    "from scipy import stats\n",
    "from sklearn.linear_model import LinearRegression, TheilSenRegressor\n",
    "from sklearn.model_selection import train_test_split, KFold, StratifiedKFold, GridSearchCV\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "#PyTorch built-ins\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torchvision.transforms import ToTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 683,
     "status": "ok",
     "timestamp": 1619802893218,
     "user": {
      "displayName": "Nelson Salazar",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhV5m5l7Buk-Ru2abgDPbFAk7ZlpSSCm1lxnxYz=s64",
      "userId": "11090170720674283185"
     },
     "user_tz": 300
    },
    "id": "w5rat47L5hC-",
    "outputId": "316952b3-9ab6-4ee1-833e-f27b8607972d"
   },
   "outputs": [],
   "source": [
    "# Get cpu or gpu device for training.\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using {} device\".format(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "47dkUKGytsEO"
   },
   "source": [
    "# 1. Datasets Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SysA\n",
    "file_name = '/home/UANDES/na.salazar10/Tesis/data_SysA.csv'\n",
    "data = pd.read_csv(filepath_or_buffer=file_name, \n",
    "                   sep=',',\n",
    "                   decimal='.',\n",
    "                   header='infer')\n",
    "data_SysA = pd.DataFrame(data)\n",
    "data_SysA.set_index(keys='Unnamed: 0', drop=True, inplace=True)\n",
    "data_SysA.index.name = None\n",
    "\n",
    "#SysB\n",
    "'''file_name = '/home/UANDES/na.salazar10/Tesis/data_SysB.csv'\n",
    "data = pd.read_csv(filepath_or_buffer=file_name, \n",
    "                   sep=',',\n",
    "                   decimal='.',\n",
    "                   header='infer')\n",
    "data_SysB = pd.DataFrame(data)\n",
    "data_SysB.set_index(keys='Unnamed: 0', drop=True, inplace=True)\n",
    "data_SysB.index.name = None'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove zeros irradiance rows\n",
    "'''\n",
    "index_names = data_SysA[(data_SysA['Hour'] < 6) | (data_SysA['Hour'] >= 18)].index\n",
    "data_SysA.drop(index_names, inplace=True)\n",
    "data_SysA.reset_index(inplace=True)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XNP2qX4vp45m"
   },
   "source": [
    "### 1.1. Train dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.1. K-fold splits (10 K-fold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16271,
     "status": "ok",
     "timestamp": 1619737845433,
     "user": {
      "displayName": "Nelson Salazar",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhV5m5l7Buk-Ru2abgDPbFAk7ZlpSSCm1lxnxYz=s64",
      "userId": "11090170720674283185"
     },
     "user_tz": 300
    },
    "id": "eIY5l7Br1hkQ",
    "outputId": "b3a73224-03b4-41bf-8997-f3066e34df3e"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Randomly split the dataframe into (train+val)-test\n",
    "    - df: Dataframe to be split\n",
    "    - kfolds: Scikit-learn KFold list (indexes to be split in dataframes)\n",
    "    - split: Number of K-fold split to generate dataframes, i.e. [0 - n_splits) from KFold function.\n",
    "Note: Seed of 'random_state' varies from [0 - len(kfolds)) and makes the train+val split stochastic.\n",
    "'''\n",
    "#Function definition\n",
    "def kfold_dataframes(df, kfolds, split, verbose):\n",
    "    #Seed\n",
    "    seed = np.random.randint(low=0, high=len(kfolds), size=None)\n",
    "\n",
    "    #Datasets: Train - Test from K-fold\n",
    "    train_val = df.loc[kfolds[0]]\n",
    "    test = df.loc[kfolds[1]]\n",
    "\n",
    "    #Datasets: Train 85 - Val 15\n",
    "    train, val = train_test_split(train_val, test_size=0.15, random_state=seed)\n",
    "\n",
    "    #Size checking\n",
    "    if verbose:\n",
    "        print(f'Train shape: {np.shape(train)}')\n",
    "        print(f'Val shape: {np.shape(val)}')\n",
    "        print(f'Test shape: {np.shape(test)}')\n",
    "\n",
    "    return train, val, test\n",
    "\n",
    "#Function check\n",
    "df = data_SysA\n",
    "kfolds = list(KFold(n_splits=5, shuffle=True, random_state=None).split(df))\n",
    "rand_split = np.random.randint(low=0, high=len(kfolds))\n",
    "fold = kfolds[rand_split]\n",
    "train, val, test = kfold_dataframes(df=data_SysA, kfolds=fold, split=rand_split, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.2. Set inputs (X) and targets (Y_hat) variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inputs (X) dataset\n",
    "#X_cols = ['Irrad_poa', 'Tamb', 'Tmod', 'Hour', 'Month', 'Year', 'k', 'i_mp', 'v_mp', 'p_mp', 'ac_power']\n",
    "#X_cols = ['Irrad_poa', 'Tamb', 'Tmod', 'Hour', 'Month', 'k', 'i_mp', 'v_mp', 'p_mp', 'ac_power', 'SOI_perc', 'DC_Cable_Losses_perc', 'AC_Cable_Losses_perc', 'Inverter_perc', 'Daily_Degradation_perc']\n",
    "\n",
    "X_cols = ['Irrad_poa', 'Tamb', 'Tmod', 'Hour', 'Month', 'k', 'i_mp', 'v_mp', 'p_mp', 'ac_power']\n",
    "\n",
    "#Targets (Yhat) dataset\n",
    "#Yhat_cols = ['IL', 'i_sc', 'v_oc', 'mod_eff_exp', 'inv_eff_exp']\n",
    "Yhat_cols = ['IL']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.3. Train inputs (X) and targets (Y_hat) normalization (0 to 1 scaling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scaler\n",
    "sc = MinMaxScaler(feature_range=(0,1), copy=True)\n",
    "\n",
    "def normalization(train, val, test, X_cols, Yhat_cols, scaler, scale_targets):\n",
    "    #Numpy inputs (X) datasets\n",
    "    X_train = pd.DataFrame(sc.fit_transform(train[X_cols]), columns=X_cols, index=train.index)\n",
    "    X_val = pd.DataFrame(sc.fit_transform(val[X_cols]), columns=X_cols, index=val.index)\n",
    "    X_test = pd.DataFrame(sc.fit_transform(test[X_cols]), columns=X_cols, index=test.index)\n",
    "\n",
    "    #Numpy targets (Yhat) dataset\n",
    "    if scale_targets:\n",
    "        Yhat_train = pd.DataFrame(sc.fit_transform(train[Yhat_cols]), columns=Yhat_cols, index=train.index)\n",
    "        Yhat_val = pd.DataFrame(sc.fit_transform(val[Yhat_cols]), columns=Yhat_cols, index=val.index)\n",
    "        Yhat_test = pd.DataFrame(sc.fit_transform(test[Yhat_cols]), columns=Yhat_cols, index=test.index)\n",
    "    else:\n",
    "        Yhat_train = pd.DataFrame(train[Yhat_cols], columns=Yhat_cols, index=train.index)\n",
    "        Yhat_val = pd.DataFrame(val[Yhat_cols], columns=Yhat_cols, index=val.index)\n",
    "        Yhat_test = pd.DataFrame(test[Yhat_cols], columns=Yhat_cols, index=test.index)\n",
    "    \n",
    "    return X_train, X_val, X_test, Yhat_train, Yhat_val, Yhat_test\n",
    "\n",
    "#Function check\n",
    "X_train, X_val, X_test, Yhat_train, Yhat_val, Yhat_test = normalization(train=train, \n",
    "                                                                        val=val, \n",
    "                                                                        test=test, \n",
    "                                                                        X_cols=X_cols, \n",
    "                                                                        Yhat_cols=Yhat_cols, \n",
    "                                                                        scaler=sc, \n",
    "                                                                        scale_targets=False)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.4. Numpy arrays to PyTorch Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PyTorch tensors\n",
    "def df_to_tensor(X_train, X_val, X_test, Yhat_train, Yhat_val, Yhat_test, verbose):\n",
    "    #Train\n",
    "    inputs_train = torch.tensor(X_train.values, dtype=torch.float32)\n",
    "    targets_train = torch.tensor(Yhat_train.values, dtype=torch.float32)\n",
    "\n",
    "    if verbose:\n",
    "        print('TRAIN')\n",
    "        print(f'Inputs Size: {inputs_train.size()} | dtype: {inputs_train.dtype}')\n",
    "        print(f'Targets Size: {targets_train.size()} | dtype: {targets_train.dtype} \\n')\n",
    "\n",
    "    #Val\n",
    "    inputs_val = torch.tensor(X_val.values, dtype=torch.float32)\n",
    "    targets_val = torch.tensor(Yhat_val.values, dtype=torch.float32)\n",
    "    \n",
    "    if verbose:\n",
    "        print('VAL')\n",
    "        print(f'Inputs Size: {inputs_val.size()} | dtype: {inputs_val.dtype}')\n",
    "        print(f'Targets Size: {targets_val.size()} | dtype: {targets_val.dtype} \\n')\n",
    "\n",
    "    #Test\n",
    "    inputs_test = torch.tensor(X_test.values, dtype=torch.float32)\n",
    "    targets_test = torch.tensor(Yhat_test.values, dtype=torch.float32)\n",
    "\n",
    "    if verbose:\n",
    "        print('TEST')\n",
    "        print(f'Inputs Size: {inputs_test.size()} | dtype: {inputs_test.dtype}')\n",
    "        print(f'Targets Size: {targets_test.size()} | dtype: {targets_test.dtype} \\n')\n",
    "\n",
    "    return inputs_train, targets_train, inputs_val, targets_val, inputs_test, targets_test\n",
    "\n",
    "#Function check\n",
    "inputs_train, targets_train, inputs_val, targets_val, inputs_test, targets_test = df_to_tensor(X_train=X_train, \n",
    "                                                                                               X_val=X_val, \n",
    "                                                                                               X_test=X_test, \n",
    "                                                                                               Yhat_train=Yhat_train, \n",
    "                                                                                               Yhat_val=Yhat_val, \n",
    "                                                                                               Yhat_test=Yhat_test,\n",
    "                                                                                               verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WrVQP3re3P90"
   },
   "source": [
    "### 1.2. Dataset and DataLoader\n",
    "The `TensorDataset` allows us to access a small section of the training data using the array indexing notation (`[0:3]` in the above code). It returns a tuple with two elements. The first element contains the input variables for the selected rows, and the second contains the targets.\n",
    "\n",
    "We'll also create a `DataLoader`, which can split the data into batches of a predefined size while training. It also provides other utilities like shuffling and random sampling of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 466,
     "status": "ok",
     "timestamp": 1619738402574,
     "user": {
      "displayName": "Nelson Salazar",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhV5m5l7Buk-Ru2abgDPbFAk7ZlpSSCm1lxnxYz=s64",
      "userId": "11090170720674283185"
     },
     "user_tz": 300
    },
    "id": "djIKW9DF25Z0",
    "outputId": "9786142c-bf10-4fdc-dbeb-08a0e462e7f9"
   },
   "outputs": [],
   "source": [
    "#Define datasets\n",
    "def datasets(inputs_train, targets_train, inputs_val, targets_val, inputs_test, targets_test):\n",
    "    train = TensorDataset(inputs_train, targets_train)\n",
    "    val = TensorDataset(inputs_val, targets_val)\n",
    "    test = TensorDataset(inputs_test, targets_test)\n",
    "\n",
    "    return train, val, test\n",
    "\n",
    "#Function check\n",
    "train, val, test = datasets(inputs_train=inputs_train, \n",
    "                            targets_train=targets_train, \n",
    "                            inputs_val=inputs_val, \n",
    "                            targets_val=targets_val, \n",
    "                            inputs_test=inputs_test, \n",
    "                            targets_test=targets_test)\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sX6IC6WX258N"
   },
   "outputs": [],
   "source": [
    "#Define train DataLoader\n",
    "def dataloader(train, val, test, batch_size, num_workers, pin_memory):\n",
    "    train_loader = DataLoader(dataset=train, \n",
    "                            batch_size=batch_size, \n",
    "                            shuffle=True,\n",
    "                            num_workers=num_workers,\n",
    "                            pin_memory=pin_memory)\n",
    "\n",
    "    val_loader = DataLoader(dataset=val, \n",
    "                            batch_size=batch_size*2, \n",
    "                            shuffle=False,\n",
    "                            num_workers=num_workers,\n",
    "                            pin_memory=pin_memory)\n",
    "\n",
    "    test_loader = DataLoader(dataset=test, \n",
    "                            batch_size=1, \n",
    "                            shuffle=False,\n",
    "                            num_workers=num_workers,\n",
    "                            pin_memory=pin_memory)\n",
    "\n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "#Function check\n",
    "batch_size = 5000 #Datapoints that will go through the ANN\n",
    "train_loader, val_loader, test_loader = dataloader(train=train, \n",
    "                                                   val=val,\n",
    "                                                   test=test,\n",
    "                                                   batch_size=batch_size,\n",
    "                                                   num_workers=4,\n",
    "                                                   pin_memory=True)\n",
    "\n",
    "#To see the first inputs-targets from DataLoader\n",
    "for xb, yb in train_loader:\n",
    "    print('xb.device:', xb.device)\n",
    "    print('xb:', xb)\n",
    "    print('yb:', yb)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pwWZ2Gz5vt5Q"
   },
   "source": [
    "# 2. Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Get device (GPU or CPU)\n",
    "Helper function to ensure that our code uses the GPU if available and defaults to using the CPU if it isn't."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_default_device():\n",
    "    '''Pick GPU if available, else CPU'''\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device('cuda')\n",
    "    else:\n",
    "        return torch.device('cpu')\n",
    "\n",
    "device = get_default_device()\n",
    "device       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "liuoqRA1vzKZ"
   },
   "source": [
    "### 2.2. Hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ANN parameters\n",
    "input_size = inputs_train.size()[1]\n",
    "hidden1_size = 12 #[6, 9, 12, 15]\n",
    "hidden2_size = 12 #[6, 9, 12, 15]\n",
    "output_size = targets_train.size()[1]\n",
    "dropout = 0.2\n",
    "\n",
    "#Train parameters\n",
    "num_epochs = 300\n",
    "learning_rate = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. ANN Design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining the model\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden1_size, hidden2_size, output_size):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        \n",
    "        #First layer: Linear -> ReLU -> Dropout\n",
    "        self.l1 = nn.Linear(in_features=input_size, out_features=hidden1_size, bias=True)\n",
    "        self.activ1 = nn.LeakyReLU()\n",
    "        self.dropout1 = nn.Dropout(p=dropout, inplace=False)\n",
    "\n",
    "        #Second layer: Linear -> ReLU -> Dropout\n",
    "        self.l2 = nn.Linear(in_features=hidden1_size, out_features=hidden2_size, bias=True)\n",
    "        self.activ2 = nn.Sigmoid()\n",
    "        self.dropout2 = nn.Dropout(p=dropout, inplace=False)\n",
    "\n",
    "        #Third layer: Linear --> Prediction (output)\n",
    "        self.l3 = nn.Linear(in_features=hidden2_size, out_features=output_size, bias=True)\n",
    "    \n",
    "    def forward(self, xb):\n",
    "        out = self.l1(xb)\n",
    "        out = self.activ1(out)\n",
    "        out = self.dropout1(out)\n",
    "\n",
    "        out = self.l2(out)\n",
    "        out = self.activ2(out)\n",
    "        out = self.dropout2(out)\n",
    "\n",
    "        out = self.l3(out)\n",
    "        return out\n",
    "    \n",
    "    def predict_step(self, batch):\n",
    "        #Forward pass\n",
    "        inputs, targets = batch\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        predict = self(inputs) #Generate predictions\n",
    "        loss = criterion(predict, targets) #Calculate loss\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4. Model, Loss and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model\n",
    "model = NeuralNet(input_size=input_size,\n",
    "                  hidden1_size=hidden1_size,\n",
    "                  hidden2_size=hidden2_size,\n",
    "                  output_size=output_size).to(device)\n",
    "print(model)\n",
    "\n",
    "#Loss function (criterion)\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "#Optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), \n",
    "                             lr=learning_rate,\n",
    "                             betas=(0.9, 0.999),\n",
    "                             eps=1e-08,\n",
    "                             weight_decay=0, \n",
    "                             amsgrad=False)\n",
    "\n",
    "#Learning rate scheduler\n",
    "scheduler = lr_scheduler.ReduceLROnPlateau(optimizer=optimizer,\n",
    "                                           mode='min',\n",
    "                                           factor=0.1,\n",
    "                                           patience=(num_epochs/100)+2,\n",
    "                                           threshold=0.0001,\n",
    "                                           threshold_mode='rel',\n",
    "                                           cooldown=0,\n",
    "                                           min_lr=0,\n",
    "                                           eps=1e-08, \n",
    "                                           verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation_step(self, batch):\n",
    "        inputs, targets = batch\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "        out = self(inputs)                               #Generate predictions\n",
    "        \n",
    "        #Loss\n",
    "        criterion = torch.nn.MSELoss()\n",
    "        loss = criterion(out, targets)                   #Calculate loss\n",
    "        '''\n",
    "        copy_targets = sc.inverse_transform(targets.detach().to('cpu'))\n",
    "        copy_targets = torch.tensor(copy_targets, dtype=torch.float32)\n",
    "        copy_out = sc.inverse_transform(out.detach().to('cpu'))\n",
    "        copy_out = torch.tensor(copy_out, dtype=torch.float32)\n",
    "\n",
    "        copy_loss = criterion(copy_out, copy_targets)\n",
    "        '''\n",
    "        return {'val_loss': loss.detach()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5. Reset weights\n",
    "Try resetting model weights to avoid weight leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_weights(model):\n",
    "    for layers in model.children():\n",
    "        for layer in layers:\n",
    "            if hasattr(layer, 'reset_parameters'):\n",
    "                layer.reset_parameters()\n",
    "\n",
    "def weight_reset(model):\n",
    "    if isinstance(model, nn.Conv2d) or isinstance(model, nn.Linear):\n",
    "        model.reset_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6. Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dict to store losses\n",
    "models = {}\n",
    "stats = {'train_loss': [], 'val_loss': [], 'predictions': [], 'Yhat_test': [], 'mse': [], 'rmse': [], 'r_square': []}\n",
    "\n",
    "def fit_model(df, kfolds, model, criterion, optimizer, epochs, lr, batch_size, scheduler, X_cols, Yhat_cols, sc, scale_targets, train_loader, val_loader, verb_step, verbose):\n",
    "    #Model train and validation  \n",
    "    print('Start model training.')\n",
    "    print('#'*25)\n",
    "\n",
    "    m_name = f'{Yhat_cols[0]}_{hidden1_size}_{hidden2_size}'\n",
    "    models[m_name] = {}\n",
    "\n",
    "    for split, fold in enumerate(kfolds):\n",
    "        f_name = f'Fold_{split+1}'\n",
    "        print('-'*25)\n",
    "        print(f_name)\n",
    "        print('-'*25)\n",
    "\n",
    "        #Restart model parameters\n",
    "        #model.apply(reset_weights)\n",
    "        model.apply(weight_reset)\n",
    "\n",
    "        #Lists data\n",
    "        models[m_name][f_name] = {'train_loss': [], 'val_loss': [], 'predictions': [], 'Yhat_test': [], 'mse': [], 'rmse': [], 'r_square': []}\n",
    "        train_loss = []\n",
    "        val_loss = []\n",
    "\n",
    "        #Step 1: Train, val and test dataframes\n",
    "        train, val, test = kfold_dataframes(df=df, kfolds=fold, split=split, verbose=verb_step)\n",
    "\n",
    "        #Step 2: Normalization\n",
    "        X_train, X_val, X_test, Yhat_train, Yhat_val, Yhat_test = normalization(train=train, \n",
    "                                                                                val=val, \n",
    "                                                                                test=test, \n",
    "                                                                                X_cols=X_cols, \n",
    "                                                                                Yhat_cols=Yhat_cols, \n",
    "                                                                                scaler=sc, \n",
    "                                                                                scale_targets=scale_targets)\n",
    "        \n",
    "        #Step 3: Inputs and targets                                                                           \n",
    "        inputs_train, targets_train, inputs_val, targets_val, inputs_test, targets_test = df_to_tensor(X_train=X_train, \n",
    "                                                                                                       X_val=X_val, \n",
    "                                                                                                       X_test=X_test, \n",
    "                                                                                                       Yhat_train=Yhat_train, \n",
    "                                                                                                       Yhat_val=Yhat_val, \n",
    "                                                                                                       Yhat_test=Yhat_test,\n",
    "                                                                                                       verbose=verb_step)\n",
    "        \n",
    "        #Step 4: Dataset\n",
    "        train, val, test = datasets(inputs_train=inputs_train, \n",
    "                                    targets_train=targets_train, \n",
    "                                    inputs_val=inputs_val, \n",
    "                                    targets_val=targets_val, \n",
    "                                    inputs_test=inputs_test, \n",
    "                                    targets_test=targets_test)\n",
    "\n",
    "        #Step 5: DataLoader\n",
    "        train_loader, val_loader, test_loader = dataloader(train=train, \n",
    "                                                           val=val,\n",
    "                                                           test=test,\n",
    "                                                           batch_size=batch_size,\n",
    "                                                           num_workers=4,\n",
    "                                                           pin_memory=True)                   \n",
    "\n",
    "        #Step 6: Epoch training run\n",
    "        for epoch in tqdm(range(epochs), desc='Epochs', leave=False):\n",
    "            #Training phase\n",
    "            train_epoch_loss = 0\n",
    "\n",
    "            model.train() #Set model to training mode\n",
    "            for i, (batch) in enumerate(train_loader):\n",
    "                optimizer.zero_grad() #Clear accumulated gradients\n",
    "\n",
    "                #Forward pass\n",
    "                loss = model.predict_step(batch)\n",
    "\n",
    "                #Backward pass and optimize\n",
    "                loss.backward() #Back propagation\n",
    "                optimizer.step() #Updating parameters\n",
    "\n",
    "                #Save loss progress\n",
    "                train_epoch_loss += loss.item()\n",
    "                \n",
    "            #Validation phase\n",
    "            with torch.no_grad():\n",
    "                val_epoch_loss = 0\n",
    "\n",
    "                model.eval() #Set model to evaluate mode\n",
    "                for batch in val_loader:\n",
    "                    #Forward pass\n",
    "                    loss = model.predict_step(batch)\n",
    "                    #Save loss progress\n",
    "                    val_epoch_loss += loss.item()\n",
    "\n",
    "            models[m_name][f_name]['train_loss'].append(train_epoch_loss/len(train_loader)) #Average loss per epoch\n",
    "            models[m_name][f_name]['val_loss'].append(val_epoch_loss/len(val_loader)) #Average loss per epoch\n",
    "\n",
    "            #Print progress if verbose\n",
    "            total_steps = len(train_loader)\n",
    "            total_steps = 10\n",
    "            if verbose:\n",
    "                if (epoch+1) % total_steps == 0:\n",
    "                    lr_sch = optimizer.param_groups[0]['lr']\n",
    "                    print(f'Epoch [{epoch+1+0:03}/{num_epochs}] | Train Loss: {train_epoch_loss/len(train_loader):.4f} | Val Loss: {val_epoch_loss/len(val_loader):.4f} | LR: {lr_sch}')\n",
    "                \n",
    "                if epoch == num_epochs:\n",
    "                    print('-'*25)\n",
    "                    print('End fold training.')\n",
    "            \n",
    "            #Scheduler step\n",
    "            scheduler.step(val_epoch_loss)\n",
    "            if optimizer.param_groups[0]['lr'] <= 1e-07:\n",
    "                optimizer.param_groups[0]['lr'] = learning_rate\n",
    "        \n",
    "        #Step 7: Fold test\n",
    "        print('-'*25)\n",
    "        print('Start fold testing.')\n",
    "        print('-'*25)    \n",
    "        predict_list = []\n",
    "        Yhat_list = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            for batch in tqdm(test_loader, desc='Batch', leave=False):\n",
    "                inputs, _ = batch\n",
    "                inputs = inputs.to(device)\n",
    "                predict = model(inputs)\n",
    "                predict_list.append(predict.cpu().numpy())\n",
    "                Yhat_list.append(_.cpu().numpy())\n",
    "\n",
    "        predict_list = [x.squeeze().tolist() for x in predict_list]\n",
    "    \n",
    "        #Metrics\n",
    "        mse = mean_squared_error(y_true=Yhat_test.values, y_pred=predict_list, squared=True)\n",
    "        rmse = np.sqrt(mse)\n",
    "        r_square = r2_score(y_true=Yhat_test.values, y_pred=predict_list)\n",
    "\n",
    "        models[m_name][f_name]['predictions'].append(predict_list)\n",
    "        models[m_name][f_name]['Yhat_test'].append(Yhat_test.values)\n",
    "        models[m_name][f_name]['mse'].append(mse)\n",
    "        models[m_name][f_name]['rmse'].append(rmse)\n",
    "        models[m_name][f_name]['r_square'].append(r_square)\n",
    "\n",
    "        #Print progress if verbose\n",
    "        if verbose:\n",
    "            print(f'MSE: {mse:.4f} | RMSE: {rmse:.4f} | R2: {r_square:.4f}')\n",
    "\n",
    "        print('-'*25)\n",
    "        print('End fold testing.')\n",
    "\n",
    "        #Reset learning rate\n",
    "        optimizer.param_groups[0]['lr'] = learning_rate\n",
    "        scheduler = scheduler\n",
    "\n",
    "        #Step 8: Data download\n",
    "        #Predictions and Yhat\n",
    "        dataf = pd.DataFrame({'predictions': models[m_name][f_name]['predictions'][0]})\n",
    "        dataf['Yhat_test'] = models[m_name][f_name]['Yhat_test'][0]\n",
    "        file_name = f'/home/UANDES/na.salazar10/Tesis/Fold/{f_name}_predictions.csv'\n",
    "        dataf.to_csv(file_name, header=True, index=False, decimal='.')\n",
    "\n",
    "        #Losses\n",
    "        dataf = pd.DataFrame({'train_loss': models[m_name][f_name]['train_loss'], \n",
    "                              'val_loss': models[m_name][f_name]['val_loss']})\n",
    "        file_name = f'/home/UANDES/na.salazar10/Tesis/Fold/{f_name}_losses.csv'\n",
    "        dataf.to_csv(file_name, header=True, index=False, decimal='.')\n",
    "\n",
    "        #Metrics\n",
    "        dataf = pd.DataFrame({'mse': models[m_name][f_name]['mse'], \n",
    "                            'rmse': models[m_name][f_name]['rmse'], \n",
    "                            'r_square': models[m_name][f_name]['r_square']})\n",
    "        file_name = f'/home/UANDES/na.salazar10/Tesis/Fold/{f_name}_metrics.csv'\n",
    "        dataf.to_csv(file_name, header=True, index=False, decimal='.')\n",
    "\n",
    "    #Step 9: Save model and parameters\n",
    "    save_path = f'/home/UANDES/na.salazar10/Tesis/models/{m_name}.pth'\n",
    "    torch.save(model.state_dict(), save_path)\n",
    "\n",
    "    if split+1 == len(kfolds):\n",
    "        print('#'*25)\n",
    "        print('End model training.')    \n",
    "    \n",
    "    return models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.5.1. Model training in action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Train\n",
    "models = {}\n",
    "num_epochs = 120\n",
    "kfolds = list(KFold(n_splits=5, shuffle=True, random_state=None).split(df))\n",
    "model_trained = fit_model(df=data_SysA, \n",
    "                          kfolds=kfolds, \n",
    "                          model=model, \n",
    "                          criterion=criterion, \n",
    "                          optimizer=optimizer, \n",
    "                          epochs=num_epochs, \n",
    "                          lr=learning_rate, \n",
    "                          batch_size=batch_size, \n",
    "                          scheduler=scheduler,\n",
    "                          X_cols=X_cols,\n",
    "                          Yhat_cols=['IL'], #Yhat_cols = ['IL', 'i_sc', 'v_oc', 'mod_eff_exp', 'inv_eff_exp']\n",
    "                          sc=sc,\n",
    "                          scale_targets=False, \n",
    "                          train_loader=train_loader, \n",
    "                          val_loader=val_loader, \n",
    "                          verb_step=False,\n",
    "                          verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model name\n",
    "model = list(models.keys())[0]\n",
    "\n",
    "for i in range (5):\n",
    "    #Fold to download\n",
    "    fold = f'Fold_{i+1}'\n",
    "    \n",
    "    #Predictions and Yhat\n",
    "    dataf = pd.DataFrame({'predictions': models[model][fold]['predictions'][0]})\n",
    "    dataf['Yhat_test'] = models[model][fold]['Yhat_test'][0]\n",
    "    file_name = f'/home/UANDES/na.salazar10/Tesis/Fold/{fold}_predictions.csv'\n",
    "    dataf.to_csv(file_name, header=True, index=False, decimal='.')\n",
    "\n",
    "    #Losses\n",
    "    dataf = pd.DataFrame({'train_loss': models[model][fold]['train_loss'], \n",
    "                        'val_loss': models[model][fold]['val_loss']})\n",
    "    file_name = f'/home/UANDES/na.salazar10/Tesis/Fold/{fold}_losses.csv'\n",
    "    dataf.to_csv(file_name, header=True, index=False, decimal='.')\n",
    "\n",
    "    #Metrics\n",
    "    dataf = pd.DataFrame({'mse': models[model][fold]['mse'], \n",
    "                        'rmse': models[model][fold]['rmse'], \n",
    "                        'r_square': models[model][fold]['r_square']})\n",
    "    file_name = f'/home/UANDES/na.salazar10/Tesis/Fold/{fold}_metrics.csv'\n",
    "    dataf.to_csv(file_name, header=True, index=False, decimal='.')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "#Clean loss history\n",
    "loss_stats['train_loss'].clear(), loss_stats['val_loss'].clear()\n",
    "\n",
    "#Train\n",
    "model_trained = fit_model(model=model, \n",
    "                          criterion=criterion,\n",
    "                          optimizer=optimizer,\n",
    "                          epochs=num_epochs, \n",
    "                          lr=learning_rate,\n",
    "                          scheduler=scheduler,\n",
    "                          train_loader=train_loader,\n",
    "                          val_loader=val_loader,\n",
    "                          verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.5.2. Training plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hor = 8\n",
    "ver = 5\n",
    "plt.figure(figsize=(hor,ver))\n",
    "\n",
    "plt.plot(loss_stats['train_loss'], marker='o', ls='-', markersize=6, linewidth=1, alpha=0.5, label='Train Loss', color='#1580E4')\n",
    "plt.plot(loss_stats['val_loss'], marker='o', ls='-', markersize=6, linewidth=1, alpha=0.5, label='Validation Loss', color='orangered')\n",
    "\n",
    "plt.title('Losses Behaviour', fontsize=15)\n",
    "plt.ylabel('MSE Loss', fontsize=13)\n",
    "plt.xlabel('Epochs', fontsize=13)\n",
    "\n",
    "plt.tick_params(direction='in', length=6, width=1, grid_alpha=0.5)\n",
    "plt.xlim(0, num_epochs)\n",
    "plt.ylim(0, None)\n",
    "plt.xticks(rotation=0)\n",
    "plt.grid(True)\n",
    "plt.legend(loc='upper right', fontsize=10)\n",
    "plt.tight_layout;\n",
    "#plt.savefig('Protocolo_SD.eps', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6. Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_list = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    for batch in tqdm(test_loader, desc='Batch', leave=False):\n",
    "        inputs, _ = batch\n",
    "        inputs = inputs.to(device)\n",
    "        predict = model(inputs)\n",
    "        predict_list.append(predict.cpu().numpy())\n",
    "\n",
    "predict_list = [x.squeeze().tolist() for x in predict_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Metrics\n",
    "mse = mean_squared_error(y_true=Yhat_test.values, y_pred=predict_list, squared=True)\n",
    "rmse = np.sqrt(mse)\n",
    "r_square = r2_score(y_true=Yhat_test.values, y_pred=predict_list)\n",
    "\n",
    "print(f'MSE: {mse:.4f} | RMSE: {rmse:.4f} | R2: {r_square:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eU3lEJMivslh"
   },
   "outputs": [],
   "source": [
    "#Target-prediction check\n",
    "with torch.no_grad():\n",
    "    for i in range(0, 10):\n",
    "        inputs, targets = val[i]\n",
    "        inputs = inputs.to(device)\n",
    "        predict = model(inputs)\n",
    "        \n",
    "        print(f'Test {i} | Target: {targets.cpu().numpy()[0]:.4f} | Prediction: {predict.cpu().numpy()[0]:.4f}')\n",
    "        #print(\"Input: \", x)\n",
    "        #print(\"Target: \", sc.inverse_transform(targets))\n",
    "        #print(\"Prediction:\", sc.inverse_transform(predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Target-prediction check\n",
    "with torch.no_grad():\n",
    "    for i in range(0, 10):\n",
    "        inputs, targets = val[i]\n",
    "        inputs = inputs.to(device)\n",
    "        predict = model(inputs)\n",
    "        \n",
    "        #print(f'Test {i} | Target: {sc.inverse_transform(targets.numpy().reshape(-1, 1)):.4f} | Prediction: {sc.inverse_transform(predict.numpy().reshape(-1, 1)):.4f}')\n",
    "        #print(\"Input: \", x)\n",
    "        print(\"Target: \", sc.inverse_transform(targets))\n",
    "        print(\"Prediction:\", sc.inverse_transform(predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(predict_list, marker='.', ls='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(Yhat_test['v_oc'].values,  marker='.', ls='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(Yhat_test['v_oc'].values,  predict_list, marker='.', ls='')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "1mYsTMVppxdc",
    "H8I0uA4m1YT7",
    "ArkLDYwrCfyj",
    "sRksuiWhhheC"
   ],
   "name": "T2_ANN.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
